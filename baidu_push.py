import urllib.request
import os

# é…ç½®ä¿¡æ¯
API_URL = "http://data.zz.baidu.com/urls?site=https://gemini-vip.top&token=MkpV4it8Aq1PaVbS"
HOST = "gemini-vip.top"
MAX_PUSH_COUNT = 9  # æ¯å¤©å‰©ä½™é…é¢é¢„ä¼°ï¼Œä¿å®ˆè®¾ç½®ä¸º 9

def get_priority_urls():
    """è·å–ä¼˜å…ˆçº§æœ€é«˜çš„ URLï¼Œé¿å…è¶…å‡ºé…é¢"""
    urls = []
    base_dir = os.path.dirname(os.path.abspath(__file__))
    
    # 1. å¿…æ¨ï¼šæ ¸å¿ƒé¡µé¢
    priority_pages = ["legal.html", "index.html"]
    for page in priority_pages:
        if page == "index.html":
            urls.append(f"https://{HOST}/")
        else:
            # Clean URL: remove .html
            clean_page = page.replace(".html", "")
            urls.append(f"https://{HOST}/{clean_page}")
            
    # 2. é€‰æ¨ï¼šBlog é¡µé¢ (æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œæ¨æœ€æ–°çš„)
    blog_dir = os.path.join(base_dir, "blog")
    blog_urls = []
    if os.path.exists(blog_dir):
        files = []
        for file in os.listdir(blog_dir):
            if file.endswith(".html"):
                full_path = os.path.join(blog_dir, file)
                files.append((full_path, file))
        
        # æŒ‰ä¿®æ”¹æ—¶é—´å€’åºæ’åˆ—
        files.sort(key=lambda x: os.path.getmtime(x[0]), reverse=True)
        
        for _, file in files:
            if file == "index.html":
                blog_urls.append(f"https://{HOST}/blog/")
            else:
                # Clean URL: remove .html
                clean_file = file.replace(".html", "")
                blog_urls.append(f"https://{HOST}/blog/{clean_file}")

    # åˆå¹¶åˆ—è¡¨
    urls.extend(blog_urls)
    
    # 3. æˆªæ–­åˆ—è¡¨ï¼Œé˜²æ­¢è¶…é¢
    final_list = urls[:MAX_PUSH_COUNT]
    
    # æ‰“å°è¢«èˆå¼ƒçš„é“¾æ¥ï¼Œæ–¹ä¾¿æŸ¥çœ‹
    if len(urls) > MAX_PUSH_COUNT:
        print(f"âš ï¸ æ³¨æ„ï¼šå…±æœ‰ {len(urls)} ä¸ªé“¾æ¥ï¼Œä½†ä¸ºäº†ä¸è¶…é…é¢ï¼Œåªæ¨é€å‰ {MAX_PUSH_COUNT} ä¸ªã€‚")
        print("è¢«æš‚æ—¶å¿½ç•¥çš„é“¾æ¥ï¼š")
        for ignored in urls[MAX_PUSH_COUNT:]:
            print(f" - {ignored}")
            
    return final_list

def push_to_baidu(url_list):
    """æäº¤ URL åˆ° ç™¾åº¦ç«™é•¿å¹³å°"""
    if not url_list:
        print("æ²¡æœ‰éœ€è¦æ¨é€çš„é“¾æ¥ã€‚")
        return

    data = "\n".join(url_list).encode("utf-8")
    
    headers = {
        'User-Agent': 'curl/7.12.1',
        'Content-Type': 'text/plain'
    }
    
    req = urllib.request.Request(
        API_URL, 
        data=data, 
        headers=headers
    )
    
    print(f"\nğŸš€ æ­£åœ¨å‘ç™¾åº¦æ¨é€ {len(url_list)} ä¸ªæ ¸å¿ƒé“¾æ¥...")
    for url in url_list:
        print(f" - {url}")
        
    try:
        with urllib.request.urlopen(req) as response:
            code = response.getcode()
            result = response.read().decode("utf-8")
            print(f"\nã€ç™¾åº¦è¿”å›ç»“æœã€‘: {result}")
            
            if code == 200 and "success" in result:
                print("âœ… æ¨é€æˆåŠŸï¼")
            else:
                print(f"âš ï¸ æ¨é€å¯èƒ½å­˜åœ¨é—®é¢˜ï¼ŒçŠ¶æ€ç : {code}")
                
    except urllib.error.HTTPError as e:
        print(f"\nâŒ æäº¤å¤±è´¥: {e.code} {e.reason}")
        print(e.read().decode("utf-8"))
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {str(e)}")

if __name__ == "__main__":
    urls = get_priority_urls()
    if urls:
        push_to_baidu(urls)
